# -*- coding: utf-8 -*-
import warnings
import os
import time
import datetime
import random
import math
import requests
import pandas as pd
from tqdm import tqdm 
import config
from proxy_manager import ProxyManager
import concurrent.futures

os.environ['PYTHONWARNINGS'] = 'ignore'
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

import efinance as ef
import akshare as ak

class EastMoneyScraper:
    """ä¸œæ–¹è´¢å¯Œå…¨å¸‚åœºæ•°æ®æŠ“å– (ä¸¥ç¦ç›´è¿)"""
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "http://quote.eastmoney.com/",
            "Connection": "keep-alive"
        }
        self.pm = ProxyManager()
        self.domains = [
            "4.push2.eastmoney.com",
            "push2.eastmoney.com", 
            "push2his.eastmoney.com" 
        ]

    def _log(self, msg, callback=None):
        """å†…éƒ¨æ—¥å¿—åŒ…è£…å™¨"""
        if callback:
            callback(msg)
        else:
            print(msg)

    def _http_get_simple(self, url, timeout=5, proxy_url=None):
        # å¼ºåˆ¶æ£€æŸ¥ï¼šå¿…é¡»æœ‰ä»£ç†
        if not proxy_url:
            return None, "No Proxy Provided (Direct Access Forbidden)"

        proxies = {"http": proxy_url, "https": proxy_url}
        try:
            # ä»£ç†è¯·æ±‚è¶…æ—¶
            resp = requests.get(url, headers=self.headers, timeout=timeout, proxies=proxies)
            if resp.status_code == 200: return resp, None
            return None, f"Status {resp.status_code}"
        except Exception as e: return None, str(e)

    def _fetch_page_worker(self, args):
        page, fs, fields, proxy_url = args
        
        # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ²¡æœ‰ä»£ç†ï¼Œç›´æ¥æ‹’ç»æ‰§è¡Œ
        if not proxy_url:
            return None, 0, None

        page_size = 100
        
        # åŸŸåè½®è¯¢
        shuffled_domains = list(self.domains)
        random.shuffle(shuffled_domains)
        
        success_flag = False
        res_df = None
        res_total = 0
        
        for domain in shuffled_domains:
            url = f"http://{domain}/api/qt/clist/get?pn={page}&pz={page_size}&po=1&np=1&ut=bd1d9ddb04089700cf9c27f6f7426281&fltt=2&invt=2&wbp2u=|0|0|0|web&fid=f3&fs={fs}&fields={fields}&_"
            resp, err = self._http_get_simple(url, timeout=5, proxy_url=proxy_url)
            
            if resp:
                try:
                    data = resp.json()
                    if data and 'data' in data and 'diff' in data['data']:
                        raw_list = data['data']['diff']
                        total = data['data'].get('total', 0)
                        if raw_list is not None:
                            res_df = pd.DataFrame(raw_list)
                            res_total = total
                            success_flag = True
                            break
                except: pass
            time.sleep(0.1)

        if success_flag:
            if proxy_url: self.pm.mark_success(proxy_url)
            return res_df, res_total, proxy_url
        else:
            if proxy_url: self.pm.mark_failure(proxy_url)
            return None, 0, proxy_url

    def get_full_market_data_mt(self, test_mode=False, status_callback=None, progress_callback=None):
        """
        å…¨é‡æŠ“å– (çº¯ä»£ç†æ¨¡å¼)
        """
        fs = "m:0 t:6,m:0 t:80,m:1 t:2,m:1 t:23,m:0 t:81 s:2048"
        fields = "f12,f14,f2,f3,f9,f20,f100,f8,f10,f6,f23,f24"
        
        self._log(f"ğŸš€ [EastMoney] å¯åŠ¨å…¨é‡æŠ“å– (ä¸¥ç¦ç›´è¿)...", status_callback)
        
        # 0. é¢„æ£€ï¼šæ— ä»£ç†åˆ™ç›´æ¥ç»ˆæ­¢
        active_count = len(self.pm.active_proxies)
        if active_count == 0:
            self._log("âŒ é”™è¯¯ï¼šä»£ç†æ± ä¸ºç©ºï¼å·²é…ç½®ä¸ºç¦æ­¢ç›´è¿ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚", status_callback)
            return pd.DataFrame()

        # 1. è·å–å…ƒæ•°æ® (ç«é€Ÿæ¨¡å¼)
        self._log(f"ğŸ“¡ æ­£åœ¨è·å–å¸‚åœºå…ƒæ•°æ® (æ´»è·ƒä»£ç†: {active_count})...", status_callback)
        first_df = None
        total_count = 0
        
        BATCH_SIZE = 30
        while self.pm.active_proxies and first_df is None:
            snapshot = list(self.pm.active_proxies)
            if not snapshot: break
            batch = snapshot[:BATCH_SIZE]
            
            self._log(f"âš¡ ä»£ç†ç«é€Ÿä¸­... å‰©ä½™æ´»è·ƒ: {len(self.pm.active_proxies)}", status_callback)
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=len(batch)) as executor:
                futures = {
                    executor.submit(self._fetch_page_worker, (1, fs, fields, proxy)): proxy 
                    for proxy in batch
                }
                for future in concurrent.futures.as_completed(futures):
                    try:
                        res_df, res_total, _ = future.result()
                        if res_df is not None:
                            if first_df is None:
                                first_df = res_df
                                total_count = res_total
                    except: pass
                
                # === ä¿®å¤ç‚¹ï¼šPandas DataFrame ä¸èƒ½ç›´æ¥ç”¨ if åˆ¤æ–­ ===
                if first_df is not None: 
                    break
            
            time.sleep(0.5)

        # æ£€æŸ¥æ˜¯å¦è·å–æˆåŠŸ
        if first_df is None or first_df.empty:
            self._log("âŒ æ‰€æœ‰ä»£ç†å‡å°è¯•å¤±è´¥ï¼Œæ— æ³•è¿æ¥æœåŠ¡å™¨ã€‚å·²åœæ­¢ã€‚", status_callback)
            return pd.DataFrame()
        
        if test_mode:
            return self._clean_df(first_df)

        page_size = 100
        total_pages = math.ceil(total_count / page_size)
        self._log(f"ğŸ“Š å¸‚åœºæ€»æ•°: {total_count}ï¼Œå…± {total_pages} é¡µã€‚å¯åŠ¨å¹¶å‘ä¸‹è½½...", status_callback)

        # 2. å¹¶å‘ä¸‹è½½
        all_data = [first_df]
        pending_pages = list(range(2, total_pages + 1))
        
        max_rounds = 10
        current_round = 1
        
        if progress_callback: progress_callback(0.0)
        pages_done = 1 

        while pending_pages and current_round <= max_rounds:
            proxy_count = len(self.pm.active_proxies)
            
            # ä¸¥æ ¼æ£€æŸ¥ï¼šå¦‚æœæ²¡æœ‰ä»£ç†äº†ï¼Œç›´æ¥é€€å‡ºå¾ªç¯ï¼Œä¸å°è¯•ç›´è¿
            if proxy_count == 0:
                self._log("âš ï¸ ä»£ç†æ± å·²è€—å°½ï¼Œåœæ­¢åç»­æŠ“å–ã€‚", status_callback)
                break

            max_workers = min(proxy_count * 2, 50)
            if max_workers < 1: max_workers = 1
            max_workers = min(max_workers, len(pending_pages))
            
            self._log(f"ğŸ”„ Round {current_round}: è¡¥å½• {len(pending_pages)} é¡µ (Threads={max_workers})...", status_callback)

            failed_pages = []
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {}
                for p in pending_pages:
                    # æ¯æ¬¡å–ä¸€ä¸ªä»£ç†ï¼Œå¦‚æœå–ä¸åˆ°(None)ï¼Œfetch_workerä¼šç›´æ¥è¿”å›å¤±è´¥
                    proxy = self.pm.get_proxy() 
                    ft = executor.submit(self._fetch_page_worker, (p, fs, fields, proxy))
                    futures[ft] = p
                
                for future in concurrent.futures.as_completed(futures):
                    p = futures[future]
                    try:
                        res_df, _, _ = future.result()
                        if res_df is not None:
                            all_data.append(res_df)
                            pages_done += 1
                            if progress_callback:
                                progress = min(pages_done / total_pages, 1.0)
                                progress_callback(progress)
                        else:
                            failed_pages.append(p)
                    except:
                        failed_pages.append(p)
            
            pending_pages = failed_pages
            current_round += 1
            if pending_pages: time.sleep(1)
        
        full_df = pd.concat(all_data, ignore_index=True)
        full_df = full_df.drop_duplicates(subset=['f12'])
        
        completion_rate = len(full_df)/total_count*100
        self._log(f"âœ… æŠ“å–ç»“æŸã€‚å®è·: {len(full_df)} (è¦†ç›–ç‡ {completion_rate:.1f}%)", status_callback)
        
        if progress_callback: progress_callback(1.0)
        
        return self._clean_df(full_df)

    def _clean_df(self, df):
        rename_map = {
            'f12': 'code', 'f14': 'name', 'f2': 'price', 'f3': 'pct_chg',
            'f9': 'pe', 'f20': 'market_cap', 'f100': 'industry',
            'f8': 'turnover', 'f10': 'volume_ratio', 'f6': 'amount',
            'f23': 'pb', 'f24': 'pct_60d'
        }
        df = df.rename(columns=rename_map)
        df = df[df['price'].astype(str) != '-'] 
        for col in ['pe', 'market_cap', 'pb', 'pct_60d', 'price', 'pct_chg', 'turnover', 'amount']:
            if col in df.columns:
                df[col] = df[col].astype(str).replace('-', '0')
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        return df

class MarketUpdater:
    def __init__(self):
        self.repo_dir = config.DATA_REPO
        self.snapshot_file = os.path.join(self.repo_dir, 'market_snapshot_full.csv')
        self.industry_map_file = os.path.join(self.repo_dir, 'industry_map.csv')
        self.financial_file = os.path.join(self.repo_dir, 'financial_map.csv')
        self.scraper = EastMoneyScraper()

    def _log(self, msg, callback=None):
        if callback: callback(msg)
        else: print(msg)

    # ä»£ç†æ‰§è¡Œå™¨ (ä¸¥æ ¼æ¨¡å¼ï¼šå¤±è´¥ä¸ç›´è¿)
    def _run_with_proxy(self, func, **kwargs):
        max_retries = 20
        retries = 0
        old_http = os.environ.get('http_proxy')
        old_https = os.environ.get('https_proxy')

        while retries < max_retries:
            proxy_url = self.scraper.pm.get_proxy()
            if not proxy_url: break # ä»£ç†æ± ç©ºï¼Œç›´æ¥é€€å‡º

            os.environ['http_proxy'] = proxy_url
            os.environ['https_proxy'] = proxy_url
            try:
                df = func(**kwargs)
                if df is not None:
                    if proxy_url: self.scraper.pm.mark_success(proxy_url)
                    self._restore_env(old_http, old_https)
                    return df
            except:
                if proxy_url: self.scraper.pm.mark_failure(proxy_url)
            retries += 1
        
        # å¾ªç¯ç»“æŸï¼Œè¿˜åŸç¯å¢ƒï¼Œå¹¶è¿”å› None (ä¸æ‰§è¡Œ func å…œåº•)
        self._restore_env(old_http, old_https)
        return None

    def _restore_env(self, old_http, old_https):
        if old_http: os.environ['http_proxy'] = old_http
        else: os.environ.pop('http_proxy', None)
        if old_https: os.environ['https_proxy'] = old_https
        else: os.environ.pop('https_proxy', None)

    def update_financial_data(self, test_mode=False, status_callback=None):
        self._log("ğŸ”„ [ä»»åŠ¡4] æ‹‰å–è´¢åŠ¡æ•°æ® (çº¯ä»£ç†æ¨¡å¼)...", status_callback)
        if test_mode: return

        try:
            df = self._run_with_proxy(ef.stock.get_all_company_performance)
            if df is not None and not df.empty:
                rename_map = {}
                for col in df.columns:
                    if "ä»£ç " in col: rename_map[col] = "code"
                    elif "å‡€åˆ©æ¶¦" in col and "å¢é•¿" in col: rename_map[col] = "profit_yoy"
                    elif "è¥ä¸šæ”¶å…¥" in col and "å¢é•¿" in col: rename_map[col] = "revenue_yoy"
                    elif "å‡€èµ„äº§æ”¶ç›Šç‡" in col: rename_map[col] = "roe"
                    elif "æ¯›åˆ©ç‡" in col: rename_map[col] = "gross_margin"
                df = df.rename(columns=rename_map)
                df = df[['code', 'profit_yoy', 'revenue_yoy', 'roe', 'gross_margin']]
                df['code'] = df['code'].astype(str).str.zfill(6)
                for c in ['profit_yoy', 'revenue_yoy', 'roe', 'gross_margin']:
                    df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)
                
                df.to_csv(self.financial_file, index=False)
                self._log(f"âœ… è´¢åŠ¡æ•°æ®æ›´æ–°æˆåŠŸã€‚", status_callback)
            else:
                self._log("âŒ è´¢åŠ¡æ•°æ®æ‹‰å–å¤±è´¥ (ä»£ç†æ± æ— æ•ˆ)ã€‚", status_callback)
        except Exception as e:
            self._log(f"âŒ å¼‚å¸¸: {e}", status_callback)

    def update_market_snapshot(self, test_mode=False, status_callback=None, progress_callback=None):
        self._log(f"ğŸ”„ [ä»»åŠ¡1] æ›´æ–°å…¨å¸‚åœºå¿«ç…§ (çº¯ä»£ç†æ¨¡å¼)...", status_callback)
        
        df_base = self.scraper.get_full_market_data_mt(
            test_mode=test_mode, 
            status_callback=status_callback, 
            progress_callback=progress_callback
        )
        
        if df_base.empty:
            self._log("âŒ æ— æ³•è·å–è¡Œæƒ…æ•°æ®ï¼Œæ›´æ–°ä¸­æ­¢ã€‚", status_callback)
            return

        self._log(f"ğŸ“‹ æ­£åœ¨åˆå¹¶æ•°æ®...", status_callback)
        df_base['code'] = df_base['code'].astype(str).str.zfill(6)

        if not test_mode:
            self._log("ğŸ§¬ è¡¥å……è‚¡æ¯ç‡ (AkShare+Proxy)...", status_callback)
            try:
                # åŒæ ·ä½¿ç”¨ _run_with_proxyï¼Œæ— ä»£ç†åˆ™è·³è¿‡
                df_ak = self._run_with_proxy(ak.stock_zh_a_spot_em)
                if df_ak is not None and not df_ak.empty:
                    rename_ak = {}
                    for col in df_ak.columns:
                        if 'ä»£ç ' in col: rename_ak[col] = 'code'
                        elif 'è‚¡æ¯' in col: rename_ak[col] = 'dividend_yield'
                    df_ak = df_ak.rename(columns=rename_ak)
                    if 'dividend_yield' in df_ak.columns:
                        df_ak = df_ak[['code', 'dividend_yield']]
                        df_ak['code'] = df_ak['code'].astype(str).str.zfill(6)
                        df_base = pd.merge(df_base, df_ak, on='code', how='left')
            except: pass

        if os.path.exists(self.financial_file):
            try:
                df_fin = pd.read_csv(self.financial_file, dtype={'code': str})
                df_base = pd.merge(df_base, df_fin, on='code', how='left')
            except: pass

        target_cols = ['code', 'name', 'price', 'pct_chg', 'pe', 'market_cap', 'industry', 'turnover', 'amount', 'volume_ratio', 'pct_60d', 'profit_yoy', 'revenue_yoy', 'roe', 'gross_margin', 'pb', 'dividend_yield']
        for c in target_cols:
            if c not in df_base.columns:
                if c == 'industry': df_base[c] = 'å…¶ä»–'
                elif c in ['name', 'code']: df_base[c] = '-'
                else: df_base[c] = 0

        df_base['industry'] = df_base['industry'].fillna('å…¶ä»–').replace(['-', '0', 0], 'å…¶ä»–')
        
        if not test_mode:
            try:
                df_map = df_base[['code', 'industry']].drop_duplicates('code')
                df_map.to_csv(self.industry_map_file, index=False)
            except: pass

        df_base['update_time'] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        df_base.to_csv(self.snapshot_file, index=False)
        self._log(f"âœ… å¿«ç…§æ›´æ–°æˆåŠŸï¼å·²ä¿å­˜è‡³ {self.snapshot_file}", status_callback)

    def update_all_kline_incremental(self, days_back=365): pass

if __name__ == "__main__":
    print("\nğŸš€ Kronos  æ•°æ®ä¸­å¿ƒ (çº¯ä»£ç†+Bugä¿®å¤ç‰ˆ)")
    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ [1.å…¨é‡ / 2.æµ‹è¯•]: ").strip()
    updater = MarketUpdater()
    is_test = (choice == '2')
    updater.update_financial_data(test_mode=is_test)
    updater.update_market_snapshot(test_mode=is_test)